<html>
  <body>
    <p>URL to analyze:</p>
    <input type="file" id="analyze"/>
    <canvas width="500" height="500" id="canvas" />
  </body>

  <script src="./face-api.js"></script>
  <script type="text/javascript">
  let DB_SERVER;
  let facedata;
  let ready = false;
  let labeledFaceDescriptors;

  const main = async () => {
    let config = await fetch(`/config`).then(r => r.json());
    DB_SERVER = config.DB_SERVER;

    let faceData = await fetch(`${DB_SERVER}/facedata`).then(r => r.json());
    const MODELS_PATH = `/models`;
    await faceapi.loadSsdMobilenetv1Model(MODELS_PATH);
    await faceapi.loadFaceLandmarkModel(MODELS_PATH);
    await faceapi.loadFaceRecognitionModel(MODELS_PATH);
    await faceapi.loadFaceExpressionModel(MODELS_PATH);

    labeledFaceDescriptors = faceData.map(desc => {
      let arr = desc.faceDescriptors.map(fd => {
        return new Float32Array([...Object.values(fd)]); 
      });
      return new faceapi.LabeledFaceDescriptors(desc.label, arr);
    });

    console.log(labeledFaceDescriptors[0]);

    ready = true;
  };

  main();

  let file = document.querySelector("#analyze");

  const loadImage = () => {  
    if (!ready) {
      alert("Models and configuration not loaded yet, please wait a minute");
      return;
    }
    let canvas = document.querySelector("canvas");
    let ctx = canvas.getContext("2d");
    // if (!props.file) {
    //   ctx.clearRect(0, 0, canvas.width, canvas.height);
    //   return;
    // }
    let reader = new FileReader();
    reader.onload = function(event){
        let img = new Image();
        img.onload = async function(){
          let imageRatio = img.width / img.height;
          canvas.height = 500;
          canvas.width = 500 * imageRatio;
          ctx.drawImage(img, 0, 0, img.width, img.height, 0, 0, canvas.width, canvas.height);
          let input = img;
          let fullFaceDescriptions = await faceapi
            .detectAllFaces(input)
            .withFaceLandmarks()
            .withFaceDescriptors()
            .withFaceExpressions();
          let dim = new faceapi.Dimensions(canvas.width, canvas.height);
          fullFaceDescriptions = faceapi.resizeResults(fullFaceDescriptions, dim);
          // if(!props.recognition) faceapi.draw.drawDetections(canvas, fullFaceDescriptions);
          // if(!props.recognition && props.showLandmarks) faceapi.draw.drawFaceLandmarks(canvas, fullFaceDescriptions);
          // if(!props.recognition && props.showExpressions) faceapi.draw.drawFaceExpressions(canvas, fullFaceDescriptions);
          // if (props.recognition) {
            const maxDescriptorDistance = 0.6;
            const faceMatcher = new faceapi.FaceMatcher(labeledFaceDescriptors, maxDescriptorDistance);
            
            const results = fullFaceDescriptions.map(fd => faceMatcher.findBestMatch(fd.descriptor));
            results.forEach((bestMatch, i) => {
              const box = fullFaceDescriptions[i].detection.box;
              const text = bestMatch.toString();
              const drawBox = new faceapi.draw.DrawBox(box, { label: text });
              drawBox.draw(canvas);
            });
          // }
        }
        img.src = event.target.result;
    }
    if (file) {
      reader.readAsDataURL(file.files[0]);
    } 
  }


  file.addEventListener("change", loadImage);

  </script>
</html>